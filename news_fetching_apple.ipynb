{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acf6be02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            publisheddate  title_polarity  title_subjectivity  \\\n",
      "0    2024-07-03T14:26:50Z        0.000000            0.000000   \n",
      "1    2024-06-25T10:45:40Z        0.700000            0.600000   \n",
      "2    2024-06-24T10:07:41Z        0.000000            0.000000   \n",
      "3    2024-06-21T19:01:40Z        0.000000            0.000000   \n",
      "4    2024-06-21T03:32:37Z       -0.155556            0.288889   \n",
      "..                    ...             ...                 ...   \n",
      "118  2023-07-17T17:11:52Z        0.000000            0.000000   \n",
      "119  2023-07-15T14:00:32Z        0.136364            0.454545   \n",
      "120  2023-07-12T11:48:59Z        0.000000            0.000000   \n",
      "121  2023-07-05T17:37:27Z       -0.400000            0.400000   \n",
      "122  2023-07-05T16:00:37Z        0.000000            0.100000   \n",
      "\n",
      "     headline_polarity  headline_subjectivity  content_polarity  \\\n",
      "0             0.000000               0.000000          0.032146   \n",
      "1             0.700000               0.600000          0.193186   \n",
      "2             0.000000               0.000000          0.101725   \n",
      "3             0.000000               0.000000          0.062386   \n",
      "4            -0.155556               0.288889          0.089154   \n",
      "..                 ...                    ...               ...   \n",
      "118           0.000000               0.000000          0.183333   \n",
      "119           0.136364               0.454545          0.143049   \n",
      "120           0.000000               0.000000         -0.012218   \n",
      "121          -0.400000               0.400000         -0.077778   \n",
      "122           0.000000               0.100000         -0.002347   \n",
      "\n",
      "     content_subjectivity  \n",
      "0                0.476854  \n",
      "1                0.501167  \n",
      "2                0.362370  \n",
      "3                0.321705  \n",
      "4                0.378750  \n",
      "..                    ...  \n",
      "118              0.642063  \n",
      "119              0.475145  \n",
      "120              0.418759  \n",
      "121              0.411111  \n",
      "122              0.362592  \n",
      "\n",
      "[123 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import concurrent.futures\n",
    "from textblob import TextBlob\n",
    "\n",
    "API_KEY = \"abd46575-1136-4a4b-8868-d5d223a9f1e1\"\n",
    "BASE_URL = \"https://content.guardianapis.com/technology/apple?from-date=2023-07-01&api-key={}&type=article&page=\".format(API_KEY)\n",
    "\n",
    "def get_urls(base_url, num_pages):\n",
    "    return [base_url + str(i) for i in range(1, num_pages + 1)]\n",
    "\n",
    "def fetch_data(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching data from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def fetch_article_content(api_url):\n",
    "    try:\n",
    "        response = requests.get(api_url + \"?api-key=\" + API_KEY + \"&show-fields=body,headline\")\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        content = data['response']['content']['fields'].get('body', 'No content available')\n",
    "        headline = data['response']['content']['fields'].get('headline', 'No headline available')\n",
    "        return content, headline\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching article content from {api_url}: {e}\")\n",
    "        return None, None\n",
    "    except KeyError as e:\n",
    "        print(f\"Error parsing article content: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def extract_info(json_data):\n",
    "    if not json_data:\n",
    "        return []\n",
    "    try:\n",
    "        articles = json_data['response']['results']\n",
    "        return [\n",
    "            {\n",
    "                'title': article['webTitle'],\n",
    "                'sectionname': article['sectionName'],\n",
    "                'publisheddate': article['webPublicationDate'],\n",
    "                'api_url': article['apiUrl']\n",
    "            } for article in articles\n",
    "        ]\n",
    "    except KeyError as e:\n",
    "        print(f\"Error parsing data: {e}\")\n",
    "        return []\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity, blob.sentiment.subjectivity\n",
    "\n",
    "def main():\n",
    "    urls = get_urls(BASE_URL, 13)\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        json_data_list = list(executor.map(fetch_data, urls))\n",
    "    \n",
    "    info = []\n",
    "    for json_data in json_data_list:\n",
    "        info.extend(extract_info(json_data))\n",
    "    \n",
    "    if info:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            content_and_headlines = list(executor.map(fetch_article_content, [article['api_url'] for article in info]))\n",
    "        \n",
    "        for i, (content, headline) in enumerate(content_and_headlines):\n",
    "            if content and headline:\n",
    "                info[i]['content'] = content\n",
    "                info[i]['headline'] = headline\n",
    "            info[i].pop('api_url')  # Remove api_url from the final data\n",
    "\n",
    "        df = pd.DataFrame(info)\n",
    "\n",
    "        # Perform sentiment analysis on title, headline, and content\n",
    "        df['title_polarity'], df['title_subjectivity'] = zip(*df['title'].map(analyze_sentiment))\n",
    "        df['headline_polarity'], df['headline_subjectivity'] = zip(*df['headline'].map(analyze_sentiment))\n",
    "        df['content_polarity'], df['content_subjectivity'] = zip(*df['content'].map(analyze_sentiment))\n",
    "        \n",
    "        # Select relevant columns for displaying\n",
    "        sentiment_df = df[['publisheddate', 'title_polarity', 'title_subjectivity', \n",
    "                           'headline_polarity', 'headline_subjectivity', \n",
    "                           'content_polarity', 'content_subjectivity']]\n",
    "        \n",
    "        # Display the sentiment analysis DataFrame with dates\n",
    "        print(sentiment_df)\n",
    "    else:\n",
    "        print(\"No data found\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27df21d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
